# MLM+NSP（Decoder-Only）



## 1.BERT

### ***Motivation***

1.  **传统模型的 “单向性” 局限**
   * **ELMO**：虽用双向 LSTM，但本质是 “分别训练左→右和右→左两个单向模型，再拼接结果”，并非真正的 “同时利用左右上下文”
   * **GPT-1**：基于 Transformer 解码器，但为了自回归（按顺序预测下一个词），强制屏蔽了 “未来 token（右半部分上下文）”，只能利用左半部分信息
2. **预训练与下游任务的 “适配鸿沟”**
   * 早期模型（如 Word2Vec）的预训练目标是 “预测上下文词”，得到的是**静态词向量**（同一个词在不同句子中向量固定，比如 “银行” 在 “去银行取钱” 和 “河边有银行” 中向量相同），无法适配 “动态语义“
   * ELMo 这类动态向量模型，预训练目标与下游任务（如文本分类、命名实体识别）的 “任务形式差异大”，导致微调时需要大量修改模型结构，适配成本高

### Pipeline

##### Step1. Pre-training

1. **输入结构**

​					`[CLS] + Sentence A + [SEP] + Sentence B + [SEP]`

* `[CLS]`：加在句子开头，用于后续分类任务（其向量作为整个句子的 “全局表示”）
* `[SEP]`：加在句子末尾（单句任务）或两个句子之间（句对任务，如 “句子 A 是否是句子 B 的下一句”），用于分隔文本

​			`Embedding(t) = TokenEmb[t] + PositionEmb[pos] + SegmentEmb[seg]`

* **Token Embedding**：通过WordPiece切分
* **Position Embedding**：与原始Transformer不同，使用的是可学习的位置向量
* **Segment Embedding**：给句子 A 和句子 B 的 token 分别添加不同向量，区分两个句子

2. 任务1.**MLM**（**Masked Language Model**）

​	随机选择输入中 15% 的 token 进行 “掩码替换”，让模型预测被替换的原 token

- 80% 概率用 `[MASK]` 替换（如 “我喜欢吃苹果”→“我喜欢吃 [MASK]”）
- 10% 概率用随机 token 替换（如 “我喜欢吃香蕉”）
- 10% 概率保持原 token 不变（如 “我喜欢吃苹果”）

​	这样设计的目的是：强制模型必须 “结合左右双向上下文” 才能准确预测原 token（比如预测 “[MASK]” 是 “苹果”，需要同时看 “吃” 和 “喜欢” 的上下文），避免模型依赖 “单一方向信息” 或 “token 本身的特征”

3. 任务2.**NSP**（**Next Sentence Prediction**）

​	给模型输入一对句子（A, B），让模型判断 B 是否是 A 在原文中的 “真实下一句”

- 50% 概率为 “真实下一句”（正例，如 A=“猫喜欢吃鱼”，B=“鱼是猫的主食”）

- 50% 概率为 “随机句子”（负例，如 A=“猫喜欢吃鱼”，B=“天空是蓝色的”）

  目的是让模型学习 “句子间的逻辑关联”，适配下游需要句对理解的任务（如问答、文本蕴含）

##### Step2. Fine-tuning

**复用预训练的 Transformer 编码器（核心语义提取模块），仅修改输出层**，针对不同任务调整输入形式和输出头

- **单句任务（如文本分类、情感分析）**：输入单句 +`[CLS]`，用 `[CLS]` 的向量接 “分类层” 预测类别
- **句对任务（如文本蕴含、语义相似度）**：输入句对（A+`[SEP]`+B），用 `[CLS]` 的向量接 “分类层” 判断关系
- **序列标注任务（如命名实体识别、词性标注）**：输入单句，给每个 token 的向量接 “标注层”，预测每个 token 的标签（如 “B-LOC”“I-PER”）
- **问答任务（如 SQuAD）**：输入 “问题 + 段落”，用两个 “预测头” 分别预测答案的 “起始位置” 和 “结束位置”

### Limitations

尽管 BERT 开启了 NLP 的 “预训练时代”，但受限于当时的技术设计和算力，仍存在以下关键局限：

#### 1. 预训练目标的 “偏差问题”

- **MLM 的 “掩码伪影”**：预训练时用 `[MASK]` 替换 token，但下游任务中不存在 `[MASK]`，导致 “预训练与微调的输入分布不一致”（模型在预训练中依赖 “`[MASK]` 是待预测 token” 的信号，下游任务中无此信号，可能影响性能）
- **NSP 的 “任务简单性”**：NSP 本质是 “判断句子是否相关”，但任务难度过低（负例多为完全无关的句子），模型容易通过 “主题是否一致”（如 “猫” 和 “天空” 主题不同）轻松判断，难以学到深层的 “句子逻辑关联”（后续模型如 RoBERTa 已证明移除 NSP 能提升性能）

#### 2. 模型效率与规模的 “矛盾”

- **参数量与算力需求高**：BERT 基础版（BERT-Base）有 110M 参数，大型版（BERT-Large）有 340M 参数，训练需要大量算力（如 TPU 集群），且推理速度慢（Transformer 编码器的注意力计算复杂度为 *O*(*n*2)，n 是输入序列长度，长文本处理效率低）
- **序列长度限制**：BERT 最大输入序列长度固定为 512 个 token（受限于 Transformer 的算力成本），无法处理长文本（如文档级 NLP 任务、长对话），超过 512 的文本需要截断，导致丢失上下文信息

#### 3. 语义理解的 “深度不足”

- **缺乏 “推理能力”**：BERT 虽能捕捉上下文语义关联，但无法处理复杂的 “逻辑推理” 任务（如 “如果 A→B，B→C，那么 A→C”），对需要 “多步思考” 的任务（如数学推理、复杂问答）表现不佳
- **动态语义的 “局限性”**：虽比 Word2Vec 更动态，但 BERT 的语义表示仍依赖 “局部上下文”（512token 内），无法捕捉 “跨文档的语义关联”，且对 “一词多义” 的区分能力仍有限（如 “苹果” 在 “手机品牌” 和 “水果” 场景的区分，依赖更丰富的上下文）

#### 4. 多模态与领域适配的 “不足”

- **纯文本模型**：BERT 仅处理文本数据，无法融合图像、音频等多模态信息，而现实场景中 NLP 任务常需要多模态输入（如 “看图说话”“视频字幕生成”）
- **领域适配成本高**：BERT 基于通用文本（Wikipedia）预训练，在医疗、法律等专业领域的表现不佳，需要大量领域标注数据进行 “二次预训练”（如 BioBERT、LegalBERT），适配成本高





## 2.RoBERTa

### Motivation

1. **BERT训练不足**：BERT 在预训练时采用了固定的训练步数和较小规模的训练数据
2. **静态掩码（Static Mask）**：BERT生成一次Mask后，训练过程中固定
3. **NSP有效性存疑**：由于NSP任务较为简单，可能非必须，甚至有负面影响
4. **词表选择**：BERT采用WordPiece，而GPT-2采用了更灵活的BPE（Byte-Pair Encoding）

### Pipeline

##### Step1. Pre-training

1. **dataset**：BookCorpus + Wikipedia + CC-News + OpenWebText +Stories，整合规模 160GB 左右的无监督文本
2. **Dynamic Masking**：在每次训练前都会重新生成掩码样本，每次向前时随机选15% Token Mask，并按照（80% `[MASK]`，10% random token，10% unchanged）的比例，增加了训练数据的多样性
3. **Large Batch**：使用8k~32k batch size，16~64GPU并行，学习率在1e-4~6e-4，训练几十万step
4. **vocab**：采用GPT-2的BPE
5. **无NSP**：输入可以是单段或两个段拼接（随机切断），不进行句子关系的预测

##### Step2. Fine-tuning

​	和 BERT 类似

### Limitations

- **计算资源需求大**：为了使用大规模数据和更复杂的训练策略，RoBERTa 需要大量的计算资源进行训练
- **长文本处理能力有限**：虽然 RoBERTa 在性能上有提升，但本质上依然基于 Transformer 架构，Transformer 的注意力机制计算复杂度为 *O*(*n^2*)（n 为输入序列长度），这导致 RoBERTa 在处理长文本时，内存消耗大且推理速度慢，难以高效处理超长文本（如整本书籍、长文档）
- **缺乏多模态融合能力**：RoBERTa 和 BERT 一样，主要是针对文本数据进行预训练和处理，在当今多模态信息交互频繁的场景下，无法直接处理图像、音频等其他模态的数据，限制了其在多模态任务中的应用，如视频内容理解、图文交互问答等
- **领域适应问题**：尽管在通用领域表现出色，但在一些专业领域（如医疗、金融、法律），由于专业术语、语言表达和逻辑推理方式的特殊性，RoBERTa 需要大量的领域特定数据进行进一步的预训练或微调才能达到理想的效果，否则在这些领域的表现会大打折扣





## 3.ALBERT

### Motivation

​	关注如何在保证BERT性能的同时，大幅减少模型参数与训练资源消耗

### Pipeline

 	1. **Vocab Embedding因式分解**

​	在 BERT 中，词嵌入维度（E）和 Transformer 隐层维度（H）相同，导致参数量大。ALBERT 指出，词嵌入层主要学习语法和词面信息，Transformer 层关注上下文语义，**二者维度不必相同**

​	因此，先将词映射到一个较低维 E，再用一个（E×H）的投影矩阵转换到 Transformer 隐层维 H。这样，参数量从 V×H（V 为词表大小）减少为 **V×E + E×H**，当 E 远小于 H 时，能大幅减少参数，同时对性能影响不大 

 1. **跨层参数共享**

    ALBERT 提出了两种跨层参数共享策略

    * 共享前馈网络（FFN）参数
    * 共享所有参数（包括注意力机制和 FFN）

​	通过参数共享，减少了模型中需要学习的参数数量，避免过拟合，同时在一定程度上提升了模型的泛化能力

 	1. **NSP改为SOP（Sentence Order Prediction）**

​	从同一文档中选取两个连续的句子，正例是原文顺序，负例是颠倒顺序，让模型判断句子顺序。SOP 任务更关注句子间的连贯性和语义相关性，能让模型学习到更深层次的语义信息

### Limitations

- **性能提升存在瓶颈**
- **跨层参数共享的负面影响**

​	跨层参数共享虽然减少了参数数量，但可能会限制模型在不同层学习到多样化特征的能力。不同层可能需要捕捉不同层次和类型的语义信息，参数共享可能导致某些层无法充分学习到独特的特征表示，在一定程度上影响模型对复杂语义的理解和处理能力。





## 4.Span BERT

### Motivation

1. BERT 对 “跨度级语义” 的捕捉能力弱
2. 下游跨度级任务需求多

### Pipeline

1. **跨度掩码（Span Masking）：替换 “单个 token” 为 “连续跨度”**

​	将Token-level Masking改为Span-level Masking

- 步骤 1：确定跨度长度。遵循 “几何分布” 随机生成跨度长度（长度为 1 的概率最高，长度为 2、3 的概率依次降低，最长为 10），模拟自然语言中 “短跨度更常见” 的特点（如实体多为 2-3 个 token，动作短语多为 1-2 个 token）

- 步骤 2：选择跨度起始位置。在文本序列中随机选择一个起始 token，按生成的跨度长度，确定连续的 token 组成 “目标跨度”（如序列 “我在北京大学读书” 中，随机选择起始 token “北”，跨度长度 2，得到跨度 “北京”）

- 步骤 3：掩码整个跨度。对选中的连续跨度，用

  单个特殊符号 `[MASK]` 统一替换（而非每个 token 都用`[MASK]`），例如：

  原文本：`我 在 北 京 大 学 读 书`

  跨度掩码后：`我 在 [MASK] 大 学 读 书`（掩码 “北京” 跨度）

  这种设计强制模型：预测被掩码的跨度时，必须 “结合跨度前后的上下文”，同时 “建模跨度内 token 的依赖关系”（比如预测`[MASK]`是 “北京”，需要知道 “北” 和 “京” 的绑定关系）

1. **跨度边界预测（Span Boundary Objective, SBO）：增强跨度与上下文的关联**

- 对被掩码的跨度，保留其**边界 token 的隐状态**（即跨度的第一个 token 和最后一个 token 的向量，如掩码 “北京” 时，保留 “北” 和 “京” 的隐状态）

- 让模型通过 “边界 token 隐状态 + 上下文隐状态”，预测整个被掩码跨度的 “原始 token 序列”

  例如，已知边界 token “北” 和 “京” 的隐状态，结合上下文 “我在... 大学读书”，预测中间被掩码的内容是 “北京”

  SBO 任务的核心是：让模型学会 “**通过跨度边界信息和上下文，还原跨度内容**”，从而更精准地捕捉 “跨度与上下文的语义关联”

### Limitations

1. 计算成本与 BERT 相当，未实现轻量化

2. 跨度长度与类型的泛化性有限

3. 仍依赖 “静态掩码”，数据多样性不足





## 5.XLNet

### Motivation

##### 1. 解决 BERT 的 “掩码伪影” 与 “上下文偏置”

​	BERT 采用自编码范式（通过 `[MASK]` 预测被掩盖 token），存在两个关键问题：

- **掩码伪影（Masking Artifact）**：预训练时输入含 `[MASK]`，但下游任务中无此符号，导致 “预训练与微调的输入分布不一致”—— 模型在预训练中依赖 “`[MASK]` 是待预测目标” 的信号，下游任务中失去该信号，可能影响性能
- **上下文利用不充分**：BERT 虽能利用双向上下文，但本质是 “对局部 token 的独立预测”（每个 `[MASK]` 单独预测），无法建模 “token 间的全局依赖关系”（如句子中多个实体的逻辑关联）

##### 2. 弥补传统自回归模型的 “单向性局限”

​	传统自回归模型（如 GPT-1）通过 “左→右” 或 “右→左” 顺序预测 token，能建模全局依赖，但存在 “上下文获取不完整” 问题：

- 只能利用 “已预测一侧” 的上下文（如左→右预测时，无法利用右侧未预测 token 的信息），导致对 “双向语义关联” 的捕捉能力弱（如歧义句中，右侧上下文对理解左侧 token 至关重要）

##### 3. 适配 “长文本建模” 需求

​	BERT 受限于 Transformer 注意力的 \(O(n^2)\) 复杂度（n 为序列长度），最大输入长度仅 512 个 token，无法处理长文本（如文档级问答、长对话）。XLNet 希望通过优化注意力机制，提升长文本建模能力

### Pipeline

​	LNet 保留 “预训练 - 微调” 范式，核心创新是 **“排列语言模型（Permutation Language Model, PLM）”** 和 **“双流注意力（Two-Stream Attention）”**，同时引入 “相对位置编码” 优化长文本处理：

##### Step1. Pre-training

1. **排列语言模型（PLM）：用 “排列顺序” 替代 “固定顺序”**传统自回归模型按 “左→右” 固定顺序预测，XLNet 则通过 “随机排列输入序列的 token 顺序”，让模型在每个排列下 “从已预测 token（任意方向）预测未预测 token”，从而间接利用双向上下文：

   - 步骤 1：对长度为 T 的输入序列，随机生成一个 “排列顺序”（如原序列 [x1, x2, x3, x4]，生成排列 [x3, x1, x4, x2]）

   - 步骤 2：按排列顺序划分 “已预测集（Past）” 和 “待预测集（Future）”—— 例如排列中第 k 个 token，其 “Past” 是排列中前 k-1 个 token（可能来自原序列的左 / 右两侧），“Future” 是后 T-k 个 token

   - 步骤 3：让模型基于 “Past” 预测 “排列中第 k 个 token”，通过多轮不同排列的训练，让模型学会 “从任意方向的上下文预测 token”，从而间接实现 “双向上下文建模”

     例：原序列 “小明在北京大学读书”，某排列为 “北京大学 小明 在 读书”；预测 “小明” 时，“Past” 是 “北京大学”（来自原序列右侧），“Future” 是 “在 读书”（来自原序列右侧）—— 模型通过该排列，利用了原序列右侧的 “北京大学” 信息，实现双向上下文利用

2. **双流注意力（Two-Stream Attention）：解决 “信息泄露”**排列语言模型中，若直接用普通注意力，模型可能 “看到待预测 token 的信息”（信息泄露），因此 XLNet 设计两个独立的隐状态流，分工控制 “能看哪些信息”：

   - Content Stream（内容流，用`h`表示）：

     包含 token 的内容信息 + 位置信息，作为注意力的`Key`和`Value`

      负责提供 “已预测 token 的语义内容”，为预测提供依据；允许看到 “自身 token 的内容”（因需作为上下文载体）

   - Query Stream（查询流，用 `g` 表示）：

​		仅包含 token 的位置信息（无内容信息），作为注意力的`Query`

​		负责 “预测待预测 token”，通过 “仅看位置不看内容” 避免信息泄露；不允许看到 “自身 token 的内		容”（确保预测时不依赖自身信息）

​		例：预测排列中第 k 个 token 时，`Query`用$g_k$（仅位置信息），`Key/Value`用 “Past” 中 token 的		`h`（内容 + 位置信息）—— 模型只能基于 “Past 的内容” 和 “待预测 token 的位置” 做预测，无信		息泄露

3. **相对位置编码（Relative Positional Encoding）：优化长文本处理**为突破 BERT 的 512 token 长度限制，XLNet 改进位置编码方式：

   - 传统 Transformer 用 “绝对位置编码”（每个位置对应固定向量），长序列中位置向量易冲突

   - XLNet 用 “相对位置编码”—— 注意力计算时，不依赖 token 的绝对位置，而是计算 “两个 token 之间的相对距离”（如 token A 和 token B 相距 3 个位置），并将相对距离融入注意力权重

     这一设计让模型能处理最长 1024 个 token 的序列，且注意力计算更高效，适配长文本建模需求

##### Step2. Fine-tuning

- **单句任务（如文本分类）**：用 `Content Stream` 的全局隐状态（如序列首个 token 的`h`）接分类层，无需额外修改
- **句对任务（如文本蕴含）**：输入句对（A+`[SEP]`+B），用 `Content Stream` 的全局隐状态判断关系
- **跨度级任务（如问答）**：用 `Query Stream` 定位答案跨度的起始 / 结束位置 —— 因预训练已擅长 “基于上下文预测 token”，无需额外优化
- **长文本任务（如文档摘要）**：利用相对位置编码支持 1024 token 输入，直接处理长文档。

### Limitations

1. 计算成本高，训练难度大

2. 排列策略的 “泛化性局限”

3. 长文本处理仍有瓶颈





## 6.DeBERTa

### Motivation

1. **BERT/RoBERTa 的 “注意力纠缠” 问题**

​	传统 BERT 中，Transformer 注意力机制的 **“词嵌入” 包含 “内容信息（词义）” 和 “位置信息（词在序列中的位置）”**，二者被 “捆绑” 在同一个向量中

2. **预训练与微调的 “解码差异”**

​	BERT/RoBERTa 的预训练任务（MLM）是 “预测被掩码的单个 token”，本质是 “局部语义恢复”；而下游任务（如文本分类、问答）常需要 “全局语义解码”（如从整句语义中判断类别、从长文本中定位答案）。这种 “预训练（局部恢复）与微调（全局解码）的目标差异”，导致模型在下游任务中需要额外适配，无法高效利用预训练学到的语义表示

3. **长文本处理与性能的进一步优化**

### Pipeline

​	核心创新是 **“解耦注意力（Disentangled Attention）”** 和 **“增强解码机制（Enhanced Decoding）”**，同时沿用并优化了 RoBERTa 的动态掩码策略

1. **解耦注意力（Disentangled Attention）：分离 “内容” 与 “位置” 关联**

​	将传统注意力拆分为 **“内容注意力（Content Attention）”** 和 **“位置注意力（Position Attention）”**，分别建模 “词义关联” 和 “位置关联”：

- **内容注意力**：仅基于 “词的内容向量” 计算注意力权重，衡量 “两个词的词义相似度”（如 “北京” 与 “上海” 的内容注意力权重高，因二者都是城市）

- **位置注意力**：仅基于 “词的位置向量” 计算注意力权重，衡量 “两个词的位置相近度”（如 “北京” 与 “是” 的位置注意力权重高，因二者相邻）

- **最终注意力权重 = 内容注意力权重 × 位置注意力权重（元素 - wise 相乘）**，让模型清晰区分 “语义关联” 和 “位置关联”，避免混淆

  例：在 “北京是中国首都，上海是中国经济中心” 中，“北京” 与 “上海” 的内容注意力权重高（词义关联）、位置注意力权重低（位置较远），最终权重平衡二者，准确捕捉 “同属中国重要城市” 的语义关联

2. **增强解码机制（Enhanced Decoding）：适配下游全局任务**

​	为解决 “预训练局部恢复与微调全局解码” 的差异，DeBERTa 在预训练中引入 **“解码层（Decoding Layer）”**：

- 在 Transformer 编码器输出后，新增一个 “解码层”，该层的输入是 “编码器的隐状态”，输出是 “全局语义表示”

- 预训练时，解码层不仅要预测被掩码的 token（局部恢复），还要通过 “全局语义建模” 优化隐状态（如让解码层输出的向量能同时反映局部 token 和整句语义）

  这种设计让预训练直接学习 “全局解码能力”，微调时无需大幅修改模型结构，即可快速适配文本分类、问答等全局任务

3. **优化的动态掩码与长文本支持**

- 沿用 RoBERTa 的 “动态掩码”（每轮训练重新生成掩码样本，提升数据多样性），并进一步优化掩码比例（从 15% 调整为更灵活的比例，适配不同语义密度的文本）
- 扩展最大序列长度至 1024 token（部分版本支持 2048），通过优化注意力计算的内存占用（如分片计算），提升长文本处理能力

### Limitations

1. 解耦注意力的 “过度拆分” 风险